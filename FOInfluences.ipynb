{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dAnEW5kn65Zq"
      },
      "outputs": [],
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import absolute_import\n",
        "from __future__ import unicode_literals\n",
        "\n",
        "import abc\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import linear_model, preprocessing, cluster\n",
        "import scipy.linalg as slin\n",
        "import scipy.sparse.linalg as sparselin\n",
        "import scipy.sparse as sparse\n",
        "from scipy.special import expit\n",
        "\n",
        "import os.path\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import math\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#hessians.py\n",
        "\n",
        "### Adapted from TF repo\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import gradients\n",
        "from tensorflow.python.framework import ops\n",
        "from tensorflow.python.ops import array_ops\n",
        "from tensorflow.python.ops import math_ops\n",
        "\n",
        "\n",
        "def hessian_vector_product(ys, xs, v):\n",
        "  \"\"\"Multiply the Hessian of `ys` wrt `xs` by `v`.\n",
        "  This is an efficient construction that uses a backprop-like approach\n",
        "  to compute the product between the Hessian and another vector. The\n",
        "  Hessian is usually too large to be explicitly computed or even\n",
        "  represented, but this method allows us to at least multiply by it\n",
        "  for the same big-O cost as backprop.\n",
        "  Implicit Hessian-vector products are the main practical, scalable way\n",
        "  of using second derivatives with neural networks. They allow us to\n",
        "  do things like construct Krylov subspaces and approximate conjugate\n",
        "  gradient descent.\n",
        "  Example: if `y` = 1/2 `x`^T A `x`, then `hessian_vector_product(y,\n",
        "  x, v)` will return an expression that evaluates to the same values\n",
        "  as (A + A.T) `v`.\n",
        "  Args:\n",
        "    ys: A scalar value, or a tensor or list of tensors to be summed to\n",
        "        yield a scalar.\n",
        "    xs: A list of tensors that we should construct the Hessian over.\n",
        "    v: A list of tensors, with the same shapes as xs, that we want to\n",
        "       multiply by the Hessian.\n",
        "  Returns:\n",
        "    A list of tensors (or if the list would be length 1, a single tensor)\n",
        "    containing the product between the Hessian and `v`.\n",
        "  Raises:\n",
        "    ValueError: `xs` and `v` have different length.\n",
        "  \"\"\"\n",
        "\n",
        "  # Validate the input\n",
        "  length = len(xs)\n",
        "  if len(v) != length:\n",
        "    raise ValueError(\"xs and v must have the same length.\")\n",
        "\n",
        "  # First backprop\n",
        "  grads = gradients(ys, xs)\n",
        "\n",
        "  # grads = xs\n",
        "\n",
        "  assert len(grads) == length\n",
        "\n",
        "  elemwise_products = [\n",
        "      math_ops.multiply(grad_elem, array_ops.stop_gradient(v_elem))\n",
        "      for grad_elem, v_elem in zip(grads, v) if grad_elem is not None\n",
        "  ]\n",
        "\n",
        "  # Second backprop\n",
        "  grads_with_none = gradients(elemwise_products, xs)\n",
        "  return_grads = [\n",
        "      grad_elem if grad_elem is not None \\\n",
        "      else tf.zeros_like(x) \\\n",
        "      for x, grad_elem in zip(xs, grads_with_none)]\n",
        "\n",
        "  return return_grads\n",
        "\n",
        "\n",
        "def _AsList(x):\n",
        "  return x if isinstance(x, (list, tuple)) else [x]\n",
        "\n",
        "def hessians(ys, xs, name=\"hessians\", colocate_gradients_with_ops=False,\n",
        "            gate_gradients=False, aggregation_method=None):\n",
        "  \"\"\"Constructs the Hessian of sum of `ys` with respect to `x` in `xs`.\n",
        "  `hessians()` adds ops to the graph to output the Hessian matrix of `ys`\n",
        "  with respect to `xs`.  It returns a list of `Tensor` of length `len(xs)`\n",
        "  where each tensor is the Hessian of `sum(ys)`. This function currently\n",
        "  only supports evaluating the Hessian with respect to (a list of) one-\n",
        "  dimensional tensors.\n",
        "  The Hessian is a matrix of second-order partial derivatives of a scalar\n",
        "  tensor (see https://en.wikipedia.org/wiki/Hessian_matrix for more details).\n",
        "  Args:\n",
        "    ys: A `Tensor` or list of tensors to be differentiated.\n",
        "    xs: A `Tensor` or list of tensors to be used for differentiation.\n",
        "    name: Optional name to use for grouping all the gradient ops together.\n",
        "      defaults to 'hessians'.\n",
        "    colocate_gradients_with_ops: See `gradients()` documentation for details.\n",
        "    gate_gradients: See `gradients()` documentation for details.\n",
        "    aggregation_method: See `gradients()` documentation for details.\n",
        "  Returns:\n",
        "    A list of Hessian matrices of `sum(y)` for each `x` in `xs`.\n",
        "  Raises:\n",
        "    LookupError: if one of the operations between `xs` and `ys` does not\n",
        "      have a registered gradient function.\n",
        "    ValueError: if the arguments are invalid or not supported. Currently,\n",
        "      this function only supports one-dimensional `x` in `xs`.\n",
        "  \"\"\"\n",
        "  xs = _AsList(xs)\n",
        "  kwargs = {\n",
        "      'colocate_gradients_with_ops': colocate_gradients_with_ops,\n",
        "      'gate_gradients': gate_gradients,\n",
        "      'aggregation_method': aggregation_method\n",
        "    }\n",
        "  # Compute a hessian matrix for each x in xs\n",
        "  hessians = []\n",
        "  for i, x in enumerate(xs):\n",
        "    # Check dimensions\n",
        "    ndims = x.get_shape().ndims\n",
        "    if ndims is None:\n",
        "      raise ValueError('Cannot compute Hessian because the dimensionality of '\n",
        "                       'element number %d of `xs` cannot be determined' % i)\n",
        "    elif ndims != 1:\n",
        "      raise ValueError('Computing hessians is currently only supported for '\n",
        "                       'one-dimensional tensors. Element number %d of `xs` has '\n",
        "                       '%d dimensions.' % (i, ndims))\n",
        "    with ops.name_scope(name + '_first_derivative'):\n",
        "      # Compute the partial derivatives of the input with respect to all\n",
        "      # elements of `x`\n",
        "      _gradients = tf.gradients(ys, x, **kwargs)[0]\n",
        "      # Unpack the gradients into a list so we can take derivatives with\n",
        "      # respect to each element\n",
        "      _gradients = array_ops.unpack(_gradients)\n",
        "    with ops.name_scope(name + '_second_derivative'):\n",
        "      # Compute the partial derivatives with respect to each element of the list\n",
        "      _hess = [tf.gradients(_gradient, x, **kwargs)[0] for _gradient in _gradients]\n",
        "      # Pack the list into a matrix and add to the list of hessians\n",
        "      hessians.append(array_ops.pack(_hess, name=name))\n",
        "  return hessians"
      ],
      "metadata": {
        "id": "YD52hAeE9LW7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset.py\n",
        "\n",
        "# Adapted from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/datasets/mnist.py\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class DataSet(object):\n",
        "\n",
        "    def __init__(self, x, labels):\n",
        "\n",
        "        if len(x.shape) > 2:\n",
        "            x = np.reshape(x, [x.shape[0], -1])\n",
        "\n",
        "        assert(x.shape[0] == labels.shape[0])\n",
        "\n",
        "        x = x.astype(np.float32)\n",
        "\n",
        "        self._x = x\n",
        "        self._x_batch = np.copy(x)\n",
        "        self._labels = labels\n",
        "        self._labels_batch = np.copy(labels)\n",
        "        self._num_examples = x.shape[0]\n",
        "        self._index_in_epoch = 0\n",
        "\n",
        "    @property\n",
        "    def x(self):\n",
        "        return self._x\n",
        "\n",
        "    @property\n",
        "    def labels(self):\n",
        "        return self._labels\n",
        "\n",
        "    @property\n",
        "    def num_examples(self):\n",
        "        return self._num_examples\n",
        "\n",
        "    def reset_batch(self):\n",
        "        self._index_in_epoch = 0\n",
        "        self._x_batch = np.copy(self._x)\n",
        "        self._labels_batch = np.copy(self._labels)\n",
        "\n",
        "    def next_batch(self, batch_size):\n",
        "        assert batch_size <= self._num_examples\n",
        "\n",
        "        start = self._index_in_epoch\n",
        "        self._index_in_epoch += batch_size\n",
        "        if self._index_in_epoch > self._num_examples:\n",
        "\n",
        "            # Shuffle the data\n",
        "            perm = np.arange(self._num_examples)\n",
        "            np.random.shuffle(perm)\n",
        "            self._x_batch = self._x_batch[perm, :]\n",
        "            self._labels_batch = self._labels_batch[perm]\n",
        "\n",
        "            # Start next epoch\n",
        "            start = 0\n",
        "            self._index_in_epoch = batch_size\n",
        "\n",
        "        end = self._index_in_epoch\n",
        "        return self._x_batch[start:end], self._labels_batch[start:end]\n",
        "\n",
        "\n",
        "def filter_dataset(X, Y, pos_class, neg_class):\n",
        "    \"\"\"\n",
        "    Filters out elements of X and Y that aren't one of pos_class or neg_class\n",
        "    then transforms labels of Y so that +1 = pos_class, -1 = neg_class.\n",
        "    \"\"\"\n",
        "    assert(X.shape[0] == Y.shape[0])\n",
        "    assert(len(Y.shape) == 1)\n",
        "\n",
        "    Y = Y.astype(int)\n",
        "\n",
        "    pos_idx = Y == pos_class\n",
        "    neg_idx = Y == neg_class\n",
        "    Y[pos_idx] = 1\n",
        "    Y[neg_idx] = -1\n",
        "    idx_to_keep = pos_idx | neg_idx\n",
        "    X = X[idx_to_keep, ...]\n",
        "    Y = Y[idx_to_keep]\n",
        "    return (X, Y)\n",
        "\n",
        "\n",
        "def find_distances(target, X, theta=None):\n",
        "    assert len(X.shape) == 2, \"X must be 2D, but it is currently %s\" % len(X.shape)\n",
        "    target = np.reshape(target, -1)\n",
        "    assert X.shape[1] == len(target), \\\n",
        "      \"X (%s) and target (%s) must have same feature dimension\" % (X.shape[1], len(target))\n",
        "\n",
        "    if theta is None:\n",
        "        return np.linalg.norm(X - target, axis=1)\n",
        "    else:\n",
        "        theta = np.reshape(theta, -1)\n",
        "\n",
        "        # Project onto theta\n",
        "        return np.abs((X - target).dot(theta))"
      ],
      "metadata": {
        "id": "2bAo11z9DF0o"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import absolute_import\n",
        "from __future__ import unicode_literals\n",
        "\n",
        "import abc\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import linear_model, preprocessing, cluster\n",
        "\n",
        "import scipy.linalg as slin\n",
        "import scipy.sparse.linalg as sparselin\n",
        "import scipy.sparse as sparse\n",
        "from scipy.optimize import fmin_ncg\n",
        "\n",
        "import os.path\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.ops import array_ops\n",
        "from keras import backend as K\n",
        "#from tensorflow.contrib.learn.python.learn.datasets import base  // not supported any more\n",
        "\n",
        "# from influence.hessians import hessian_vector_product\n",
        "#from influence.dataset import DataSet\n",
        "\n",
        "\n",
        "def variable(name, shape, initializer):\n",
        "    dtype = tf.float32\n",
        "    var = tf.get_variable(\n",
        "        name,\n",
        "        shape,\n",
        "        initializer=initializer,\n",
        "        dtype=dtype)\n",
        "    return var\n",
        "\n",
        "def variable_with_weight_decay(name, shape, stddev, wd):\n",
        "    \"\"\"Helper to create an initialized Variable with weight decay.\n",
        "    Note that the Variable is initialized with a truncated normal distribution.\n",
        "    A weight decay is added only if one is specified.\n",
        "    Args:\n",
        "      name: name of the variable\n",
        "      shape: list of ints\n",
        "      stddev: standard deviation of a truncated Gaussian\n",
        "      wd: add L2Loss weight decay multiplied by this float. If None, weight\n",
        "          decay is not added for this Variable.\n",
        "    Returns:\n",
        "      Variable Tensor\n",
        "    \"\"\"\n",
        "    dtype = tf.float32\n",
        "    var = variable(\n",
        "        name,\n",
        "        shape,\n",
        "        initializer=tf.truncated_normal_initializer(\n",
        "            stddev=stddev,\n",
        "            dtype=dtype))\n",
        "\n",
        "    if wd is not None:\n",
        "      weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
        "      tf.add_to_collection('losses', weight_decay)\n",
        "\n",
        "    return var\n",
        "\n",
        "def normalize_vector(v):\n",
        "    \"\"\"\n",
        "    Takes in a vector in list form, concatenates it to form a single vector,\n",
        "    normalizes it to unit length, then returns it in list form together with its norm.\n",
        "    \"\"\"\n",
        "    norm_val = np.linalg.norm(np.concatenate(v))\n",
        "    norm_v = [a/norm_val for a in v]\n",
        "    return norm_v, norm_val\n",
        "\n",
        "\n",
        "class GenericNeuralNet(object):\n",
        "    \"\"\"\n",
        "    Multi-class classification.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        np.random.seed(0)\n",
        "        tf.set_random_seed(0)\n",
        "\n",
        "        self.batch_size = kwargs.pop('batch_size')\n",
        "        self.data_sets = kwargs.pop('data_sets')\n",
        "        self.train_dir = kwargs.pop('train_dir', 'output')\n",
        "        log_dir = kwargs.pop('log_dir', 'log')\n",
        "        self.model_name = kwargs.pop('model_name')\n",
        "        self.num_classes = kwargs.pop('num_classes')\n",
        "        self.initial_learning_rate = kwargs.pop('initial_learning_rate')\n",
        "        self.decay_epochs = kwargs.pop('decay_epochs')\n",
        "\n",
        "        if 'keep_probs' in kwargs: self.keep_probs = kwargs.pop('keep_probs')\n",
        "        else: self.keep_probs = None\n",
        "\n",
        "        if 'mini_batch' in kwargs: self.mini_batch = kwargs.pop('mini_batch')\n",
        "        else: self.mini_batch = True\n",
        "\n",
        "        if 'damping' in kwargs: self.damping = kwargs.pop('damping')\n",
        "        else: self.damping = 0.0\n",
        "\n",
        "        if not os.path.exists(self.train_dir):\n",
        "            os.makedirs(self.train_dir)\n",
        "\n",
        "        # Initialize session\n",
        "        config = tf.ConfigProto()\n",
        "        self.sess = tf.Session(config=config)\n",
        "        K.set_session(self.sess)\n",
        "\n",
        "        # Setup input\n",
        "        self.input_placeholder, self.labels_placeholder = self.placeholder_inputs()\n",
        "        self.num_train_examples = self.data_sets.train.labels.shape[0]\n",
        "        self.num_test_examples = self.data_sets.test.labels.shape[0]\n",
        "\n",
        "        # Setup inference and training\n",
        "        if self.keep_probs is not None:\n",
        "            self.keep_probs_placeholder = tf.placeholder(tf.float32, shape=(2))\n",
        "            self.logits = self.inference(self.input_placeholder, self.keep_probs_placeholder)\n",
        "        elif hasattr(self, 'inference_needs_labels'):\n",
        "            self.logits = self.inference(self.input_placeholder, self.labels_placeholder)\n",
        "        else:\n",
        "            self.logits = self.inference(self.input_placeholder)\n",
        "\n",
        "        self.total_loss, self.loss_no_reg, self.indiv_loss_no_reg = self.loss(\n",
        "            self.logits,\n",
        "            self.labels_placeholder)\n",
        "\n",
        "        self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
        "        self.learning_rate = tf.Variable(self.initial_learning_rate, name='learning_rate', trainable=False)\n",
        "        self.learning_rate_placeholder = tf.placeholder(tf.float32)\n",
        "        self.update_learning_rate_op = tf.assign(self.learning_rate, self.learning_rate_placeholder)\n",
        "\n",
        "        self.train_op = self.get_train_op(self.total_loss, self.global_step, self.learning_rate)\n",
        "        self.train_sgd_op = self.get_train_sgd_op(self.total_loss, self.global_step, self.learning_rate)\n",
        "        self.accuracy_op = self.get_accuracy_op(self.logits, self.labels_placeholder)\n",
        "        self.preds = self.predictions(self.logits)\n",
        "\n",
        "        # Setup misc\n",
        "        self.saver = tf.train.Saver()\n",
        "\n",
        "        # Setup gradients and Hessians\n",
        "        self.params = self.get_all_params()\n",
        "        self.grad_total_loss_op = tf.gradients(self.total_loss, self.params)\n",
        "        self.grad_loss_no_reg_op = tf.gradients(self.loss_no_reg, self.params)\n",
        "        self.v_placeholder = [tf.placeholder(tf.float32, shape=a.get_shape()) for a in self.params]\n",
        "        self.u_placeholder = [tf.placeholder(tf.float32, shape=a.get_shape()) for a in self.params]\n",
        "\n",
        "        self.hessian_vector = hessian_vector_product(self.total_loss, self.params, self.v_placeholder)\n",
        "\n",
        "        self.grad_loss_wrt_input_op = tf.gradients(self.total_loss, self.input_placeholder)\n",
        "\n",
        "        # Because tf.gradients auto accumulates, we probably don't need the add_n (or even reduce_sum)\n",
        "        self.influence_op = tf.add_n(\n",
        "            [tf.reduce_sum(tf.multiply(a, array_ops.stop_gradient(b))) for a, b in zip(self.grad_total_loss_op, self.v_placeholder)])\n",
        "\n",
        "        self.grad_influence_wrt_input_op = tf.gradients(self.influence_op, self.input_placeholder)\n",
        "\n",
        "        self.checkpoint_file = os.path.join(self.train_dir, \"%s-checkpoint\" % self.model_name)\n",
        "\n",
        "        self.all_train_feed_dict = self.fill_feed_dict_with_all_ex(self.data_sets.train)\n",
        "        self.all_test_feed_dict = self.fill_feed_dict_with_all_ex(self.data_sets.test)\n",
        "\n",
        "        init = tf.global_variables_initializer()\n",
        "        self.sess.run(init)\n",
        "\n",
        "        self.vec_to_list = self.get_vec_to_list_fn()\n",
        "        self.adversarial_loss, self.indiv_adversarial_loss = self.adversarial_loss(self.logits, self.labels_placeholder)\n",
        "        if self.adversarial_loss is not None:\n",
        "            self.grad_adversarial_loss_op = tf.gradients(self.adversarial_loss, self.params)\n",
        "\n",
        "\n",
        "    def get_vec_to_list_fn(self):\n",
        "        params_val = self.sess.run(self.params)\n",
        "        self.num_params = len(np.concatenate(params_val))\n",
        "        print('Total number of parameters: %s' % self.num_params)\n",
        "\n",
        "\n",
        "        def vec_to_list(v):\n",
        "            return_list = []\n",
        "            cur_pos = 0\n",
        "            for p in params_val:\n",
        "                return_list.append(v[cur_pos : cur_pos+len(p)])\n",
        "                cur_pos += len(p)\n",
        "\n",
        "            assert cur_pos == len(v)\n",
        "            return return_list\n",
        "\n",
        "        return vec_to_list\n",
        "\n",
        "\n",
        "    def reset_datasets(self):\n",
        "        for data_set in self.data_sets:\n",
        "            if data_set is not None:\n",
        "                data_set.reset_batch()\n",
        "\n",
        "\n",
        "    def fill_feed_dict_with_all_ex(self, data_set):\n",
        "        feed_dict = {\n",
        "            self.input_placeholder: data_set.x,\n",
        "            self.labels_placeholder: data_set.labels\n",
        "        }\n",
        "        return feed_dict\n",
        "\n",
        "\n",
        "    def fill_feed_dict_with_all_but_one_ex(self, data_set, idx_to_remove):\n",
        "        num_examples = data_set.x.shape[0]\n",
        "        idx = np.array([True] * num_examples, dtype=bool)\n",
        "        idx[idx_to_remove] = False\n",
        "        feed_dict = {\n",
        "            self.input_placeholder: data_set.x[idx, :],\n",
        "            self.labels_placeholder: data_set.labels[idx]\n",
        "        }\n",
        "        return feed_dict\n",
        "\n",
        "\n",
        "    def fill_feed_dict_with_batch(self, data_set, batch_size=0):\n",
        "        if batch_size is None:\n",
        "            return self.fill_feed_dict_with_all_ex(data_set)\n",
        "        elif batch_size == 0:\n",
        "            batch_size = self.batch_size\n",
        "\n",
        "        input_feed, labels_feed = data_set.next_batch(batch_size)\n",
        "        feed_dict = {\n",
        "            self.input_placeholder: input_feed,\n",
        "            self.labels_placeholder: labels_feed,\n",
        "        }\n",
        "        return feed_dict\n",
        "\n",
        "\n",
        "    def fill_feed_dict_with_some_ex(self, data_set, target_indices):\n",
        "        input_feed = data_set.x[target_indices, :].reshape(len(target_indices), -1)\n",
        "        labels_feed = data_set.labels[target_indices].reshape(-1)\n",
        "        feed_dict = {\n",
        "            self.input_placeholder: input_feed,\n",
        "            self.labels_placeholder: labels_feed,\n",
        "        }\n",
        "        return feed_dict\n",
        "\n",
        "\n",
        "    def fill_feed_dict_with_one_ex(self, data_set, target_idx):\n",
        "        input_feed = data_set.x[target_idx, :].reshape(1, -1)\n",
        "        labels_feed = data_set.labels[target_idx].reshape(-1)\n",
        "        feed_dict = {\n",
        "            self.input_placeholder: input_feed,\n",
        "            self.labels_placeholder: labels_feed,\n",
        "        }\n",
        "        return feed_dict\n",
        "\n",
        "\n",
        "    def fill_feed_dict_manual(self, X, Y):\n",
        "        X = np.array(X)\n",
        "        Y = np.array(Y)\n",
        "        input_feed = X.reshape(len(Y), -1)\n",
        "        labels_feed = Y.reshape(-1)\n",
        "        feed_dict = {\n",
        "            self.input_placeholder: input_feed,\n",
        "            self.labels_placeholder: labels_feed,\n",
        "        }\n",
        "        return feed_dict\n",
        "\n",
        "\n",
        "    def minibatch_mean_eval(self, ops, data_set):\n",
        "\n",
        "        num_examples = data_set.num_examples\n",
        "        assert num_examples % self.batch_size == 0\n",
        "        num_iter = int(num_examples / self.batch_size)\n",
        "\n",
        "        self.reset_datasets()\n",
        "\n",
        "        ret = []\n",
        "        for i in xrange(num_iter):\n",
        "            feed_dict = self.fill_feed_dict_with_batch(data_set)\n",
        "            ret_temp = self.sess.run(ops, feed_dict=feed_dict)\n",
        "\n",
        "            if len(ret)==0:\n",
        "                for b in ret_temp:\n",
        "                    if isinstance(b, list):\n",
        "                        ret.append([c / float(num_iter) for c in b])\n",
        "                    else:\n",
        "                        ret.append([b / float(num_iter)])\n",
        "            else:\n",
        "                for counter, b in enumerate(ret_temp):\n",
        "                    if isinstance(b, list):\n",
        "                        ret[counter] = [a + (c / float(num_iter)) for (a, c) in zip(ret[counter], b)]\n",
        "                    else:\n",
        "                        ret[counter] += (b / float(num_iter))\n",
        "\n",
        "        return ret\n",
        "\n",
        "\n",
        "    def print_model_eval(self):\n",
        "        params_val = self.sess.run(self.params)\n",
        "\n",
        "        if self.mini_batch == True:\n",
        "            grad_loss_val, loss_no_reg_val, loss_val, train_acc_val = self.minibatch_mean_eval(\n",
        "                [self.grad_total_loss_op, self.loss_no_reg, self.total_loss, self.accuracy_op],\n",
        "                self.data_sets.train)\n",
        "\n",
        "            test_loss_val, test_acc_val = self.minibatch_mean_eval(\n",
        "                [self.loss_no_reg, self.accuracy_op],\n",
        "                self.data_sets.test)\n",
        "\n",
        "        else:\n",
        "            grad_loss_val, loss_no_reg_val, loss_val, train_acc_val = self.sess.run(\n",
        "                [self.grad_total_loss_op, self.loss_no_reg, self.total_loss, self.accuracy_op],\n",
        "                feed_dict=self.all_train_feed_dict)\n",
        "\n",
        "            test_loss_val, test_acc_val = self.sess.run(\n",
        "                [self.loss_no_reg, self.accuracy_op],\n",
        "                feed_dict=self.all_test_feed_dict)\n",
        "\n",
        "        print('Train loss (w reg) on all data: %s' % loss_val)\n",
        "        print('Train loss (w/o reg) on all data: %s' % loss_no_reg_val)\n",
        "\n",
        "        print('Test loss (w/o reg) on all data: %s' % test_loss_val)\n",
        "        print('Train acc on all data:  %s' % train_acc_val)\n",
        "        print('Test acc on all data:   %s' % test_acc_val)\n",
        "\n",
        "        print('Norm of the mean of gradients: %s' % np.linalg.norm(np.concatenate(grad_loss_val)))\n",
        "        print('Norm of the params: %s' % np.linalg.norm(np.concatenate(params_val)))\n",
        "\n",
        "\n",
        "\n",
        "    def retrain(self, num_steps, feed_dict):\n",
        "        for step in xrange(num_steps):\n",
        "            self.sess.run(self.train_op, feed_dict=feed_dict)\n",
        "\n",
        "\n",
        "    def update_learning_rate(self, step):\n",
        "        assert self.num_train_examples % self.batch_size == 0\n",
        "        num_steps_in_epoch = self.num_train_examples / self.batch_size\n",
        "        epoch = step // num_steps_in_epoch\n",
        "\n",
        "        multiplier = 1\n",
        "        if epoch < self.decay_epochs[0]:\n",
        "            multiplier = 1\n",
        "        elif epoch < self.decay_epochs[1]:\n",
        "            multiplier = 0.1\n",
        "        else:\n",
        "            multiplier = 0.01\n",
        "\n",
        "        self.sess.run(\n",
        "            self.update_learning_rate_op,\n",
        "            feed_dict={self.learning_rate_placeholder: multiplier * self.initial_learning_rate})\n",
        "\n",
        "\n",
        "    def train(self, num_steps,\n",
        "              iter_to_switch_to_batch=20000,\n",
        "              iter_to_switch_to_sgd=40000,\n",
        "              save_checkpoints=True, verbose=True):\n",
        "        \"\"\"\n",
        "        Trains a model for a specified number of steps.\n",
        "        \"\"\"\n",
        "        if verbose: print('Training for %s steps' % num_steps)\n",
        "\n",
        "        sess = self.sess\n",
        "\n",
        "        for step in xrange(num_steps):\n",
        "            self.update_learning_rate(step)\n",
        "\n",
        "            start_time = time.time()\n",
        "\n",
        "            if step < iter_to_switch_to_batch:\n",
        "                feed_dict = self.fill_feed_dict_with_batch(self.data_sets.train)\n",
        "                _, loss_val = sess.run([self.train_op, self.total_loss], feed_dict=feed_dict)\n",
        "\n",
        "            elif step < iter_to_switch_to_sgd:\n",
        "                feed_dict = self.all_train_feed_dict\n",
        "                _, loss_val = sess.run([self.train_op, self.total_loss], feed_dict=feed_dict)\n",
        "\n",
        "            else:\n",
        "                feed_dict = self.all_train_feed_dict\n",
        "                _, loss_val = sess.run([self.train_sgd_op, self.total_loss], feed_dict=feed_dict)\n",
        "\n",
        "            duration = time.time() - start_time\n",
        "\n",
        "            if verbose:\n",
        "                if step % 1000 == 0:\n",
        "                    # Print status to stdout.\n",
        "                    print('Step %d: loss = %.8f (%.3f sec)' % (step, loss_val, duration))\n",
        "\n",
        "            # Save a checkpoint and evaluate the model periodically.\n",
        "            if (step + 1) % 100000 == 0 or (step + 1) == num_steps:\n",
        "                if save_checkpoints: self.saver.save(sess, self.checkpoint_file, global_step=step)\n",
        "                if verbose: self.print_model_eval()\n",
        "\n",
        "\n",
        "    def load_checkpoint(self, iter_to_load, do_checks=True):\n",
        "        checkpoint_to_load = \"%s-%s\" % (self.checkpoint_file, iter_to_load)\n",
        "        self.saver.restore(self.sess, checkpoint_to_load)\n",
        "\n",
        "        if do_checks:\n",
        "            print('Model %s loaded. Sanity checks ---' % checkpoint_to_load)\n",
        "            self.print_model_eval()\n",
        "\n",
        "\n",
        "    def get_train_op(self, total_loss, global_step, learning_rate):\n",
        "        \"\"\"\n",
        "        Return train_op\n",
        "        \"\"\"\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "        train_op = optimizer.minimize(total_loss, global_step=global_step)\n",
        "        return train_op\n",
        "\n",
        "\n",
        "    def get_train_sgd_op(self, total_loss, global_step, learning_rate=0.001):\n",
        "        \"\"\"\n",
        "        Return train_sgd_op\n",
        "        \"\"\"\n",
        "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "        train_op = optimizer.minimize(total_loss, global_step=global_step)\n",
        "        return train_op\n",
        "\n",
        "\n",
        "    def get_accuracy_op(self, logits, labels):\n",
        "        \"\"\"Evaluate the quality of the logits at predicting the label.\n",
        "        Args:\n",
        "          logits: Logits tensor, float - [batch_size, NUM_CLASSES].\n",
        "          labels: Labels tensor, int32 - [batch_size], with values in the\n",
        "            range [0, NUM_CLASSES).\n",
        "        Returns:\n",
        "          A scalar int32 tensor with the number of examples (out of batch_size)\n",
        "          that were predicted correctly.\n",
        "        \"\"\"\n",
        "        correct = tf.nn.in_top_k(logits, labels, 1)\n",
        "        return tf.reduce_sum(tf.cast(correct, tf.int32)) / tf.shape(labels)[0]\n",
        "\n",
        "\n",
        "    def loss(self, logits, labels):\n",
        "\n",
        "        labels = tf.one_hot(labels, depth=self.num_classes)\n",
        "        # correct_prob = tf.reduce_sum(tf.multiply(labels, tf.nn.softmax(logits)), reduction_indices=1)\n",
        "        cross_entropy = - tf.reduce_sum(tf.multiply(labels, tf.nn.log_softmax(logits)), reduction_indices=1)\n",
        "\n",
        "        indiv_loss_no_reg = cross_entropy\n",
        "        loss_no_reg = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
        "        tf.add_to_collection('losses', loss_no_reg)\n",
        "\n",
        "        total_loss = tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
        "\n",
        "        return total_loss, loss_no_reg, indiv_loss_no_reg\n",
        "\n",
        "\n",
        "    def adversarial_loss(self, logits, labels):\n",
        "        # Computes sum of log(1 - p(y = true|x))\n",
        "        # No regularization (because this is meant to be computed on the test data)\n",
        "\n",
        "        labels = tf.one_hot(labels, depth=self.num_classes)\n",
        "        wrong_labels = (labels - 1) * -1 # Flips 0s and 1s\n",
        "        wrong_labels_bool = tf.reshape(tf.cast(wrong_labels, tf.bool), [-1, self.num_classes])\n",
        "\n",
        "        wrong_logits = tf.reshape(tf.boolean_mask(logits, wrong_labels_bool), [-1, self.num_classes - 1])\n",
        "\n",
        "        indiv_adversarial_loss = tf.reduce_logsumexp(wrong_logits, reduction_indices=1) - tf.reduce_logsumexp(logits, reduction_indices=1)\n",
        "        adversarial_loss = tf.reduce_mean(indiv_adversarial_loss)\n",
        "\n",
        "        return adversarial_loss, indiv_adversarial_loss #, indiv_wrong_prob\n",
        "\n",
        "\n",
        "    def update_feed_dict_with_v_placeholder(self, feed_dict, vec):\n",
        "        for pl_block, vec_block in zip(self.v_placeholder, vec):\n",
        "            feed_dict[pl_block] = vec_block\n",
        "        return feed_dict\n",
        "\n",
        "\n",
        "    def get_inverse_hvp(self, v, approx_type='cg', approx_params=None, verbose=True):\n",
        "        assert approx_type in ['cg', 'lissa']\n",
        "        if approx_type == 'lissa':\n",
        "            return self.get_inverse_hvp_lissa(v, **approx_params)\n",
        "        elif approx_type == 'cg':\n",
        "            return self.get_inverse_hvp_cg(v, verbose)\n",
        "\n",
        "\n",
        "    def get_inverse_hvp_lissa(self, v,\n",
        "                              batch_size=None,\n",
        "                              scale=10, damping=0.0, num_samples=1, recursion_depth=10000):\n",
        "        \"\"\"\n",
        "        This uses mini-batching; uncomment code for the single sample case.\n",
        "        \"\"\"\n",
        "        inverse_hvp = None\n",
        "        print_iter = recursion_depth / 10\n",
        "\n",
        "        for i in range(num_samples):\n",
        "            # samples = np.random.choice(self.num_train_examples, size=recursion_depth)\n",
        "\n",
        "            cur_estimate = v\n",
        "\n",
        "            for j in range(recursion_depth):\n",
        "\n",
        "                # feed_dict = fill_feed_dict_with_one_ex(\n",
        "                #   data_set,\n",
        "                #   images_placeholder,\n",
        "                #   labels_placeholder,\n",
        "                #   samples[j])\n",
        "                feed_dict = self.fill_feed_dict_with_batch(self.data_sets.train, batch_size=batch_size)\n",
        "\n",
        "                feed_dict = self.update_feed_dict_with_v_placeholder(feed_dict, cur_estimate)\n",
        "                hessian_vector_val = self.sess.run(self.hessian_vector, feed_dict=feed_dict)\n",
        "                cur_estimate = [a + (1-damping) * b - c/scale for (a,b,c) in zip(v, cur_estimate, hessian_vector_val)]\n",
        "\n",
        "                # Update: v + (I - Hessian_at_x) * cur_estimate\n",
        "                if (j % print_iter == 0) or (j == recursion_depth - 1):\n",
        "                    print(\"Recursion at depth %s: norm is %.8lf\" % (j, np.linalg.norm(np.concatenate(cur_estimate))))\n",
        "                    feed_dict = self.update_feed_dict_with_v_placeholder(feed_dict, cur_estimate)\n",
        "\n",
        "            if inverse_hvp is None:\n",
        "                inverse_hvp = [b/scale for b in cur_estimate]\n",
        "            else:\n",
        "                inverse_hvp = [a + b/scale for (a, b) in zip(inverse_hvp, cur_estimate)]\n",
        "\n",
        "        inverse_hvp = [a/num_samples for a in inverse_hvp]\n",
        "        return inverse_hvp\n",
        "\n",
        "\n",
        "    def minibatch_hessian_vector_val(self, v):\n",
        "\n",
        "        num_examples = self.num_train_examples\n",
        "        if self.mini_batch == True:\n",
        "            batch_size = 100\n",
        "            assert num_examples % batch_size == 0\n",
        "        else:\n",
        "            batch_size = self.num_train_examples\n",
        "\n",
        "        num_iter = int(num_examples / batch_size)\n",
        "\n",
        "        self.reset_datasets()\n",
        "        hessian_vector_val = None\n",
        "        for i in xrange(num_iter):\n",
        "            feed_dict = self.fill_feed_dict_with_batch(self.data_sets.train, batch_size=batch_size)\n",
        "            # Can optimize this\n",
        "            feed_dict = self.update_feed_dict_with_v_placeholder(feed_dict, v)\n",
        "            hessian_vector_val_temp = self.sess.run(self.hessian_vector, feed_dict=feed_dict)\n",
        "            if hessian_vector_val is None:\n",
        "                hessian_vector_val = [b / float(num_iter) for b in hessian_vector_val_temp]\n",
        "            else:\n",
        "                hessian_vector_val = [a + (b / float(num_iter)) for (a,b) in zip(hessian_vector_val, hessian_vector_val_temp)]\n",
        "\n",
        "        hessian_vector_val = [a + self.damping * b for (a,b) in zip(hessian_vector_val, v)]\n",
        "\n",
        "        return hessian_vector_val\n",
        "\n",
        "\n",
        "    def get_fmin_loss_fn(self, v):\n",
        "\n",
        "        def get_fmin_loss(x):\n",
        "            hessian_vector_val = self.minibatch_hessian_vector_val(self.vec_to_list(x))\n",
        "\n",
        "            return 0.5 * np.dot(np.concatenate(hessian_vector_val), x) - np.dot(np.concatenate(v), x)\n",
        "        return get_fmin_loss\n",
        "\n",
        "\n",
        "    def get_fmin_grad_fn(self, v):\n",
        "        def get_fmin_grad(x):\n",
        "            hessian_vector_val = self.minibatch_hessian_vector_val(self.vec_to_list(x))\n",
        "\n",
        "            return np.concatenate(hessian_vector_val) - np.concatenate(v)\n",
        "        return get_fmin_grad\n",
        "\n",
        "\n",
        "    def get_fmin_hvp(self, x, p):\n",
        "        hessian_vector_val = self.minibatch_hessian_vector_val(self.vec_to_list(p))\n",
        "\n",
        "        return np.concatenate(hessian_vector_val)\n",
        "\n",
        "\n",
        "    def get_cg_callback(self, v, verbose):\n",
        "        fmin_loss_fn = self.get_fmin_loss_fn(v)\n",
        "\n",
        "        def fmin_loss_split(x):\n",
        "            hessian_vector_val = self.minibatch_hessian_vector_val(self.vec_to_list(x))\n",
        "\n",
        "            return 0.5 * np.dot(np.concatenate(hessian_vector_val), x), -np.dot(np.concatenate(v), x)\n",
        "\n",
        "        def cg_callback(x):\n",
        "            # x is current params\n",
        "            v = self.vec_to_list(x)\n",
        "            idx_to_remove = 5\n",
        "\n",
        "            single_train_feed_dict = self.fill_feed_dict_with_one_ex(self.data_sets.train, idx_to_remove)\n",
        "            train_grad_loss_val = self.sess.run(self.grad_total_loss_op, feed_dict=single_train_feed_dict)\n",
        "            predicted_loss_diff = np.dot(np.concatenate(v), np.concatenate(train_grad_loss_val)) / self.num_train_examples\n",
        "\n",
        "            if verbose:\n",
        "                print('Function value: %s' % fmin_loss_fn(x))\n",
        "                quad, lin = fmin_loss_split(x)\n",
        "                print('Split function value: %s, %s' % (quad, lin))\n",
        "                print('Predicted loss diff on train_idx %s: %s' % (idx_to_remove, predicted_loss_diff))\n",
        "\n",
        "        return cg_callback\n",
        "\n",
        "\n",
        "    def get_inverse_hvp_cg(self, v, verbose):\n",
        "        fmin_loss_fn = self.get_fmin_loss_fn(v)\n",
        "        fmin_grad_fn = self.get_fmin_grad_fn(v)\n",
        "        cg_callback = self.get_cg_callback(v, verbose)\n",
        "\n",
        "        fmin_results = fmin_ncg(\n",
        "            f=fmin_loss_fn,\n",
        "            x0=np.concatenate(v),\n",
        "            fprime=fmin_grad_fn,\n",
        "            fhess_p=self.get_fmin_hvp,\n",
        "            callback=cg_callback,\n",
        "            avextol=1e-8,\n",
        "            maxiter=100)\n",
        "\n",
        "        return self.vec_to_list(fmin_results)\n",
        "\n",
        "\n",
        "    def get_test_grad_loss_no_reg_val(self, test_indices, batch_size=100, loss_type='normal_loss'):\n",
        "\n",
        "        if loss_type == 'normal_loss':\n",
        "            op = self.grad_loss_no_reg_op\n",
        "        elif loss_type == 'adversarial_loss':\n",
        "            op = self.grad_adversarial_loss_op\n",
        "        else:\n",
        "            raise ValueError('Loss must be specified')\n",
        "\n",
        "        if test_indices is not None:\n",
        "            num_iter = int(np.ceil(len(test_indices) / batch_size))\n",
        "\n",
        "            test_grad_loss_no_reg_val = None\n",
        "            for i in range(num_iter):\n",
        "                start = i * batch_size\n",
        "                end = int(min((i+1) * batch_size, len(test_indices)))\n",
        "\n",
        "                test_feed_dict = self.fill_feed_dict_with_some_ex(self.data_sets.test, test_indices[start:end])\n",
        "\n",
        "                temp = self.sess.run(op, feed_dict=test_feed_dict)\n",
        "\n",
        "                if test_grad_loss_no_reg_val is None:\n",
        "                    test_grad_loss_no_reg_val = [a * (end-start) for a in temp]\n",
        "                else:\n",
        "                    test_grad_loss_no_reg_val = [a + b * (end-start) for (a, b) in zip(test_grad_loss_no_reg_val, temp)]\n",
        "\n",
        "            test_grad_loss_no_reg_val = [a/len(test_indices) for a in test_grad_loss_no_reg_val]\n",
        "\n",
        "        else:\n",
        "            test_grad_loss_no_reg_val = self.minibatch_mean_eval([op], self.data_sets.test)[0]\n",
        "\n",
        "        return test_grad_loss_no_reg_val\n",
        "\n",
        "\n",
        "    def get_influence_on_test_loss(self, test_indices, train_idx,\n",
        "        approx_type='cg', approx_params=None, force_refresh=True, test_description=None,\n",
        "        loss_type='normal_loss',\n",
        "        X=None, Y=None):\n",
        "        # If train_idx is None then use X and Y (phantom points)\n",
        "        # Need to make sure test_idx stays consistent between models\n",
        "        # because mini-batching permutes dataset order\n",
        "\n",
        "        if train_idx is None:\n",
        "            if (X is None) or (Y is None): raise ValueError('X and Y must be specified if using phantom points.')\n",
        "            if X.shape[0] != len(Y): raise ValueError('X and Y must have the same length.')\n",
        "        else:\n",
        "            if (X is not None) or (Y is not None): raise ValueError('X and Y cannot be specified if train_idx is specified.')\n",
        "\n",
        "        test_grad_loss_no_reg_val = self.get_test_grad_loss_no_reg_val(test_indices, loss_type=loss_type)\n",
        "\n",
        "        print('Norm of test gradient: %s' % np.linalg.norm(np.concatenate(test_grad_loss_no_reg_val)))\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        if test_description is None:\n",
        "            test_description = test_indices\n",
        "\n",
        "        approx_filename = os.path.join(self.train_dir, '%s-%s-%s-test-%s.npz' % (self.model_name, approx_type, loss_type, test_description))\n",
        "        if os.path.exists(approx_filename) and force_refresh == False:\n",
        "            inverse_hvp = list(np.load(approx_filename)['inverse_hvp'])\n",
        "            print('Loaded inverse HVP from %s' % approx_filename)\n",
        "        else:\n",
        "            inverse_hvp = self.get_inverse_hvp(\n",
        "                test_grad_loss_no_reg_val,\n",
        "                approx_type,\n",
        "                approx_params)\n",
        "            np.savez(approx_filename, inverse_hvp=inverse_hvp)\n",
        "            print('Saved inverse HVP to %s' % approx_filename)\n",
        "\n",
        "        duration = time.time() - start_time\n",
        "        print('Inverse HVP took %s sec' % duration)\n",
        "\n",
        "\n",
        "\n",
        "        start_time = time.time()\n",
        "        if train_idx is None:\n",
        "            num_to_remove = len(Y)\n",
        "            predicted_loss_diffs = np.zeros([num_to_remove])\n",
        "            for counter in np.arange(num_to_remove):\n",
        "                single_train_feed_dict = self.fill_feed_dict_manual(X[counter, :], [Y[counter]])\n",
        "                train_grad_loss_val = self.sess.run(self.grad_total_loss_op, feed_dict=single_train_feed_dict)\n",
        "                predicted_loss_diffs[counter] = np.dot(np.concatenate(inverse_hvp), np.concatenate(train_grad_loss_val)) / self.num_train_examples\n",
        "\n",
        "        else:\n",
        "            num_to_remove = len(train_idx)\n",
        "            predicted_loss_diffs = np.zeros([num_to_remove])\n",
        "            for counter, idx_to_remove in enumerate(train_idx):\n",
        "                single_train_feed_dict = self.fill_feed_dict_with_one_ex(self.data_sets.train, idx_to_remove)\n",
        "                train_grad_loss_val = self.sess.run(self.grad_total_loss_op, feed_dict=single_train_feed_dict)\n",
        "                predicted_loss_diffs[counter] = np.dot(np.concatenate(inverse_hvp), np.concatenate(train_grad_loss_val)) / self.num_train_examples\n",
        "\n",
        "        duration = time.time() - start_time\n",
        "        print('Multiplying by %s train examples took %s sec' % (num_to_remove, duration))\n",
        "\n",
        "        return predicted_loss_diffs\n",
        "\n",
        "\n",
        "\n",
        "    def find_eigvals_of_hessian(self, num_iter=100, num_prints=10):\n",
        "\n",
        "        # Setup\n",
        "        print_iterations = num_iter / num_prints\n",
        "        feed_dict = self.fill_feed_dict_with_one_ex(self.data_sets.train, 0)\n",
        "\n",
        "        # Initialize starting vector\n",
        "        grad_loss_val = self.sess.run(self.grad_total_loss_op, feed_dict=feed_dict)\n",
        "        initial_v = []\n",
        "\n",
        "        for a in grad_loss_val:\n",
        "            initial_v.append(np.random.random(a.shape))\n",
        "        initial_v, _ = normalize_vector(initial_v)\n",
        "\n",
        "        # Do power iteration to find largest eigenvalue\n",
        "        print('Starting power iteration to find largest eigenvalue...')\n",
        "\n",
        "        largest_eig = norm_val\n",
        "        print('Largest eigenvalue is %s' % largest_eig)\n",
        "\n",
        "        # Do power iteration to find smallest eigenvalue\n",
        "        print('Starting power iteration to find smallest eigenvalue...')\n",
        "        cur_estimate = initial_v\n",
        "\n",
        "        for i in range(num_iter):\n",
        "            cur_estimate, norm_val = normalize_vector(cur_estimate)\n",
        "            hessian_vector_val = self.minibatch_hessian_vector_val(cur_estimate)\n",
        "            new_cur_estimate = [a - largest_eig * b for (a,b) in zip(hessian_vector_val, cur_estimate)]\n",
        "\n",
        "            if i % print_iterations == 0:\n",
        "                print(-norm_val + largest_eig)\n",
        "                dotp = np.dot(np.concatenate(new_cur_estimate), np.concatenate(cur_estimate))\n",
        "                print(\"dot: %s\" % dotp)\n",
        "            cur_estimate = new_cur_estimate\n",
        "\n",
        "        smallest_eig = -norm_val + largest_eig\n",
        "        assert dotp < 0, \"Eigenvalue calc failed to find largest eigenvalue\"\n",
        "\n",
        "        print('Largest eigenvalue is %s' % largest_eig)\n",
        "        print('Smallest eigenvalue is %s' % smallest_eig)\n",
        "        return largest_eig, smallest_eig\n",
        "\n",
        "\n",
        "    def get_grad_of_influence_wrt_input(self, train_indices, test_indices,\n",
        "        approx_type='cg', approx_params=None, force_refresh=True, verbose=True, test_description=None,\n",
        "        loss_type='normal_loss'):\n",
        "        \"\"\"\n",
        "        If the loss goes up when you remove a point, then it was a helpful point.\n",
        "        So positive influence = helpful.\n",
        "        If we move in the direction of the gradient, we make the influence even more positive,\n",
        "        so even more helpful.\n",
        "        Thus if we want to make the test point more wrong, we have to move in the opposite direction.\n",
        "        \"\"\"\n",
        "\n",
        "        # Calculate v_placeholder (gradient of loss at test point)\n",
        "        test_grad_loss_no_reg_val = self.get_test_grad_loss_no_reg_val(test_indices, loss_type=loss_type)\n",
        "\n",
        "        if verbose: print('Norm of test gradient: %s' % np.linalg.norm(np.concatenate(test_grad_loss_no_reg_val)))\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        if test_description is None:\n",
        "            test_description = test_indices\n",
        "\n",
        "        approx_filename = os.path.join(self.train_dir, '%s-%s-%s-test-%s.npz' % (self.model_name, approx_type, loss_type, test_description))\n",
        "\n",
        "        if os.path.exists(approx_filename) and force_refresh == False:\n",
        "            inverse_hvp = list(np.load(approx_filename)['inverse_hvp'])\n",
        "            if verbose: print('Loaded inverse HVP from %s' % approx_filename)\n",
        "        else:\n",
        "            inverse_hvp = self.get_inverse_hvp(\n",
        "                test_grad_loss_no_reg_val,\n",
        "                approx_type,\n",
        "                approx_params,\n",
        "                verbose=verbose)\n",
        "            np.savez(approx_filename, inverse_hvp=inverse_hvp)\n",
        "            if verbose: print('Saved inverse HVP to %s' % approx_filename)\n",
        "\n",
        "        duration = time.time() - start_time\n",
        "        if verbose: print('Inverse HVP took %s sec' % duration)\n",
        "\n",
        "        grad_influence_wrt_input_val = None\n",
        "\n",
        "        for counter, train_idx in enumerate(train_indices):\n",
        "            # Put in the train example in the feed dict\n",
        "            grad_influence_feed_dict = self.fill_feed_dict_with_one_ex(\n",
        "                self.data_sets.train,\n",
        "                train_idx)\n",
        "\n",
        "            self.update_feed_dict_with_v_placeholder(grad_influence_feed_dict, inverse_hvp)\n",
        "\n",
        "            # Run the grad op with the feed dict\n",
        "            current_grad_influence_wrt_input_val = self.sess.run(self.grad_influence_wrt_input_op, feed_dict=grad_influence_feed_dict)[0][0, :]\n",
        "\n",
        "            if grad_influence_wrt_input_val is None:\n",
        "                grad_influence_wrt_input_val = np.zeros([len(train_indices), len(current_grad_influence_wrt_input_val)])\n",
        "\n",
        "            grad_influence_wrt_input_val[counter, :] = current_grad_influence_wrt_input_val\n",
        "\n",
        "        return grad_influence_wrt_input_val\n",
        "\n",
        "\n",
        "    def update_train_x(self, new_train_x):\n",
        "        assert np.all(new_train_x.shape == self.data_sets.train.x.shape)\n",
        "        new_train = DataSet(new_train_x, np.copy(self.data_sets.train.labels))\n",
        "        # self.data_sets = base.Datasets(train=new_train, validation=self.data_sets.validation, test=self.data_sets.test)\n",
        "        self.all_train_feed_dict = self.fill_feed_dict_with_all_ex(new_train)\n",
        "        self.reset_datasets()\n",
        "\n",
        "\n",
        "    def update_train_x_y(self, new_train_x, new_train_y):\n",
        "        new_train = DataSet(new_train_x, new_train_y)\n",
        "        #self.data_sets = base.Datasets(train=new_train, validation=self.data_sets.validation, test=self.data_sets.test)\n",
        "        self.all_train_feed_dict = self.fill_feed_dict_with_all_ex(new_train)\n",
        "        self.num_train_examples = len(new_train_y)\n",
        "        self.reset_datasets()\n",
        "\n",
        "\n",
        "    def update_test_x_y(self, new_test_x, new_test_y):\n",
        "        new_test = DataSet(new_test_x, new_test_y)\n",
        "        # self.data_sets = base.Datasets(train=self.data_sets.train, validation=self.data_sets.validation, test=new_test)\n",
        "        self.all_test_feed_dict = self.fill_feed_dict_with_all_ex(new_test)\n",
        "        self.num_test_examples = len(new_test_y)\n",
        "        self.reset_datasets()"
      ],
      "metadata": {
        "id": "ojCYSqvB9XUF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import absolute_import\n",
        "from __future__ import unicode_literals\n",
        "\n",
        "import abc\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import linear_model, preprocessing, cluster\n",
        "import scipy.linalg as slin\n",
        "import scipy.sparse.linalg as sparselin\n",
        "import scipy.sparse as sparse\n",
        "\n",
        "import os.path\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import math\n",
        "\n",
        "from tensorflow.python.ops import array_ops\n",
        "\n",
        "#from influence.hessians import hessians\n",
        "#from influence.genericNeuralNet import GenericNeuralNet, variable, variable_with_weight_decay\n",
        "\n",
        "\n",
        "class LogisticRegressionWithLBFGS(GenericNeuralNet):\n",
        "\n",
        "    def __init__(self, input_dim, weight_decay, max_lbfgs_iter, **kwargs):\n",
        "        self.weight_decay = weight_decay\n",
        "        self.input_dim = input_dim\n",
        "        self.max_lbfgs_iter = max_lbfgs_iter\n",
        "\n",
        "        super(LogisticRegressionWithLBFGS, self).__init__(**kwargs)\n",
        "\n",
        "        self.set_params_op = self.set_params()\n",
        "        # self.hessians_op = hessians(self.total_loss, self.params)\n",
        "\n",
        "        # Multinomial has weird behavior when it's binary\n",
        "        C = 1.0 / (self.num_train_examples * self.weight_decay)\n",
        "        self.sklearn_model = linear_model.LogisticRegression(\n",
        "            C=C,\n",
        "            tol=1e-8,\n",
        "            fit_intercept=False,\n",
        "            solver='lbfgs',\n",
        "            multi_class='multinomial',\n",
        "            warm_start=True, #True\n",
        "            max_iter=max_lbfgs_iter)\n",
        "\n",
        "        C_minus_one = 1.0 / ((self.num_train_examples - 1) * self.weight_decay)\n",
        "        self.sklearn_model_minus_one = linear_model.LogisticRegression(\n",
        "            C=C_minus_one,\n",
        "            tol=1e-8,\n",
        "            fit_intercept=False,\n",
        "            solver='lbfgs',\n",
        "            multi_class='multinomial',\n",
        "            warm_start=True, #True\n",
        "            max_iter=max_lbfgs_iter)\n",
        "\n",
        "\n",
        "    def get_all_params(self):\n",
        "        all_params = []\n",
        "        for layer in ['softmax_linear']:\n",
        "            # for var_name in ['weights', 'biases']:\n",
        "            for var_name in ['weights']:\n",
        "                temp_tensor = tf.get_default_graph().get_tensor_by_name(\"%s/%s:0\" % (layer, var_name))\n",
        "                all_params.append(temp_tensor)\n",
        "        return all_params\n",
        "\n",
        "\n",
        "    def placeholder_inputs(self):\n",
        "        input_placeholder = tf.placeholder(\n",
        "            tf.float32,\n",
        "            shape=(None, self.input_dim),\n",
        "            name='input_placeholder')\n",
        "        labels_placeholder = tf.placeholder(\n",
        "            tf.int32,\n",
        "            shape=(None),\n",
        "            name='labels_placeholder')\n",
        "        return input_placeholder, labels_placeholder\n",
        "\n",
        "\n",
        "    def inference(self, input):\n",
        "        with tf.variable_scope('softmax_linear'):\n",
        "            weights = variable_with_weight_decay(\n",
        "                'weights',\n",
        "                [self.input_dim * self.num_classes],\n",
        "                stddev=1.0 / math.sqrt(float(self.input_dim)),\n",
        "                wd=self.weight_decay)\n",
        "            logits = tf.matmul(input, tf.reshape(weights, [self.input_dim, self.num_classes]))\n",
        "            # biases = variable(\n",
        "            #     'biases',\n",
        "            #     [self.num_classes],\n",
        "            #     tf.constant_initializer(0.0))\n",
        "            # logits = tf.matmul(input, tf.reshape(weights, [self.input_dim, self.num_classes])) + biases\n",
        "\n",
        "        self.weights = weights\n",
        "        # self.biases = biases\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "    def predictions(self, logits):\n",
        "        preds = tf.nn.softmax(logits, name='preds')\n",
        "        return preds\n",
        "\n",
        "\n",
        "    def set_params(self):\n",
        "        # See if we can automatically infer weight shape\n",
        "        self.W_placeholder = tf.placeholder(\n",
        "            tf.float32,\n",
        "            shape=[self.input_dim * self.num_classes],\n",
        "            name='W_placeholder')\n",
        "        # self.b_placeholder = tf.placeholder(\n",
        "        #     tf.float32,\n",
        "        #     shape=[self.num_classes],\n",
        "        #     name='b_placeholder')\n",
        "        set_weights = tf.assign(self.weights, self.W_placeholder, validate_shape=True)\n",
        "        return [set_weights]\n",
        "        # set_biases = tf.assign(self.biases, self.b_placeholder, validate_shape=True)\n",
        "        # return [set_weights, set_biases]\n",
        "\n",
        "\n",
        "\n",
        "    def retrain(self, num_steps, feed_dict):\n",
        "\n",
        "        self.train_with_LBFGS(\n",
        "            feed_dict=feed_dict,\n",
        "            save_checkpoints=False,\n",
        "            verbose=False)\n",
        "\n",
        "        # super(LogisticRegressionWithLBFGS, self).train(\n",
        "        #     num_steps,\n",
        "        #     iter_to_switch_to_batch=0,\n",
        "        #     iter_to_switch_to_sgd=1000000,\n",
        "        #     save_checkpoints=False, verbose=False)\n",
        "\n",
        "\n",
        "    def train(self, num_steps=None,\n",
        "              iter_to_switch_to_batch=None,\n",
        "              iter_to_switch_to_sgd=None,\n",
        "              save_checkpoints=True, verbose=True):\n",
        "\n",
        "        self.train_with_LBFGS(\n",
        "            feed_dict=self.all_train_feed_dict,\n",
        "            save_checkpoints=save_checkpoints,\n",
        "            verbose=verbose)\n",
        "\n",
        "        # super(LogisticRegressionWithLBFGS, self).train(\n",
        "        #     num_steps=500,\n",
        "        #     iter_to_switch_to_batch=0,\n",
        "        #     iter_to_switch_to_sgd=100000,\n",
        "        #     save_checkpoints=True, verbose=True)\n",
        "\n",
        "\n",
        "    def train_with_SGD(self, **kwargs):\n",
        "        super(LogisticRegressionWithLBFGS, self).train(**kwargs)\n",
        "\n",
        "\n",
        "    def train_with_LBFGS(self, feed_dict, save_checkpoints=True, verbose=True):\n",
        "        # More sanity checks to see if predictions are the same?\n",
        "\n",
        "        X_train = feed_dict[self.input_placeholder]\n",
        "        Y_train = feed_dict[self.labels_placeholder]\n",
        "        num_train_examples = len(Y_train)\n",
        "        assert len(Y_train.shape) == 1\n",
        "        assert X_train.shape[0] == Y_train.shape[0]\n",
        "\n",
        "        if num_train_examples == self.num_train_examples:\n",
        "            if verbose: print('Using normal model')\n",
        "            model = self.sklearn_model\n",
        "        elif num_train_examples == self.num_train_examples - 1:\n",
        "            if verbose: print('Using model minus one')\n",
        "            model = self.sklearn_model_minus_one\n",
        "        else:\n",
        "            raise ValueError('feed_dict has incorrect number of training examples')\n",
        "\n",
        "        # print(X_train)\n",
        "        # print(Y_train)\n",
        "        model.fit(X_train, Y_train)\n",
        "        # sklearn returns coefficients in shape num_classes x num_features\n",
        "        # whereas our weights are defined as num_features x num_classes\n",
        "        # so we have to tranpose them first.\n",
        "        W = np.reshape(model.coef_.T, -1)\n",
        "        # b = model.intercept_\n",
        "\n",
        "        params_feed_dict = {}\n",
        "        params_feed_dict[self.W_placeholder] = W\n",
        "        # params_feed_dict[self.b_placeholder] = b\n",
        "        self.sess.run(self.set_params_op, feed_dict=params_feed_dict)\n",
        "        if save_checkpoints: self.saver.save(self.sess, self.checkpoint_file, global_step=0)\n",
        "\n",
        "        if verbose:\n",
        "            print('LBFGS training took %s iter.' % model.n_iter_)\n",
        "            print('After training with LBFGS: ')\n",
        "            self.print_model_eval()"
      ],
      "metadata": {
        "id": "v3ZNXTR9J5D5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BinaryLogisticRegressionWithLBFGS(LogisticRegressionWithLBFGS):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "\n",
        "        super(BinaryLogisticRegressionWithLBFGS, self).__init__(**kwargs)\n",
        "\n",
        "        C = 1.0 / (self.num_train_examples * self.weight_decay)\n",
        "        self.sklearn_model = linear_model.LogisticRegression(\n",
        "            C=C,\n",
        "            tol=1e-8,\n",
        "            fit_intercept=False,\n",
        "            solver='lbfgs',\n",
        "            warm_start=True,\n",
        "            max_iter=1000)\n",
        "\n",
        "        C_minus_one = 1.0 / ((self.num_train_examples - 1) * self.weight_decay)\n",
        "        self.sklearn_model_minus_one = linear_model.LogisticRegression(\n",
        "            C=C_minus_one,\n",
        "            tol=1e-8,\n",
        "            fit_intercept=False,\n",
        "            solver='lbfgs',\n",
        "            warm_start=True,\n",
        "            max_iter=1000)\n",
        "\n",
        "\n",
        "    def inference(self, input):\n",
        "        with tf.variable_scope('softmax_linear'):\n",
        "            weights = variable_with_weight_decay(\n",
        "                'weights',\n",
        "                [self.input_dim],\n",
        "                stddev=1.0 / math.sqrt(float(self.input_dim)),\n",
        "                wd=self.weight_decay)\n",
        "\n",
        "            logits = tf.matmul(input, tf.reshape(weights, [self.input_dim, 1])) # + biases\n",
        "            zeros = tf.zeros_like(logits)\n",
        "            logits_with_zeros = tf.concat([zeros, logits], 1)\n",
        "\n",
        "        self.weights = weights\n",
        "\n",
        "        return logits_with_zeros\n",
        "\n",
        "\n",
        "    def set_params(self):\n",
        "        self.W_placeholder = tf.placeholder(\n",
        "            tf.float32,\n",
        "            shape=[self.input_dim],\n",
        "            name='W_placeholder')\n",
        "        set_weights = tf.assign(self.weights, self.W_placeholder, validate_shape=True)\n",
        "        return [set_weights]\n",
        "\n",
        "\n",
        "    # Special-purpose function for paper experiments\n",
        "    # that has flags for ignoring training error or Hessian\n",
        "    def get_influence_on_test_loss(self, test_indices, train_idx,\n",
        "        approx_type='cg', approx_params=None, force_refresh=True, test_description=None,\n",
        "        loss_type='normal_loss',\n",
        "        ignore_training_error=False,\n",
        "        ignore_hessian=False\n",
        "        ):\n",
        "\n",
        "        test_grad_loss_no_reg_val = self.get_test_grad_loss_no_reg_val(test_indices, loss_type=loss_type)\n",
        "\n",
        "        print('Norm of test gradient: %s' % np.linalg.norm(np.concatenate(test_grad_loss_no_reg_val)))\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        if test_description is None:\n",
        "            test_description = test_indices\n",
        "\n",
        "        approx_filename = os.path.join(self.train_dir, '%s-%s-%s-test-%s.npz' % (self.model_name, approx_type, loss_type, test_description))\n",
        "        if ignore_hessian == False:\n",
        "            if os.path.exists(approx_filename) and force_refresh == False:\n",
        "                inverse_hvp = list(np.load(approx_filename)['inverse_hvp'])\n",
        "                print('Loaded inverse HVP from %s' % approx_filename)\n",
        "            else:\n",
        "                inverse_hvp = self.get_inverse_hvp(\n",
        "                    test_grad_loss_no_reg_val,\n",
        "                    approx_type,\n",
        "                    approx_params)\n",
        "                np.savez(approx_filename, inverse_hvp=inverse_hvp)\n",
        "                print('Saved inverse HVP to %s' % approx_filename)\n",
        "        else:\n",
        "            inverse_hvp = test_grad_loss_no_reg_val\n",
        "\n",
        "        duration = time.time() - start_time\n",
        "        print('Inverse HVP took %s sec' % duration)\n",
        "\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        num_to_remove = len(train_idx)\n",
        "        predicted_loss_diffs = np.zeros([num_to_remove])\n",
        "        for counter, idx_to_remove in enumerate(train_idx):\n",
        "\n",
        "            if ignore_training_error == False:\n",
        "                single_train_feed_dict = self.fill_feed_dict_with_one_ex(self.data_sets.train, idx_to_remove)\n",
        "                train_grad_loss_val = self.sess.run(self.grad_total_loss_op, feed_dict=single_train_feed_dict)\n",
        "            else:\n",
        "                train_grad_loss_val = [-(self.data_sets.train.labels[idx_to_remove] * 2 - 1) * self.data_sets.train.x[idx_to_remove, :]]\n",
        "            predicted_loss_diffs[counter] = np.dot(np.concatenate(inverse_hvp), np.concatenate(train_grad_loss_val)) / self.num_train_examples\n",
        "\n",
        "        duration = time.time() - start_time\n",
        "        print('Multiplying by %s train examples took %s sec' % (num_to_remove, duration))\n",
        "\n",
        "        return predicted_loss_diffs\n",
        "\n",
        "\n",
        "    def get_loo_influences(self):\n",
        "\n",
        "        X_train = self.data_sets.train.x\n",
        "        Y_train = self.data_sets.train.labels * 2 - 1\n",
        "        theta = self.sess.run(self.params)[0]\n",
        "\n",
        "        # Pre-calculate inverse covariance matrix\n",
        "        n = X_train.shape[0]\n",
        "        dim = X_train.shape[1]\n",
        "        cov = np.zeros([dim, dim])\n",
        "\n",
        "        probs = expit(np.dot(X_train, theta.T))\n",
        "        weighted_X_train = np.reshape(probs * (1 - probs), (-1, 1)) * X_train\n",
        "\n",
        "        cov = np.dot(X_train.T, weighted_X_train) / n\n",
        "        cov += self.weight_decay * np.eye(dim)\n",
        "\n",
        "        cov_lu_factor = slin.lu_factor(cov)\n",
        "\n",
        "        assert(len(Y_train.shape) == 1)\n",
        "        x_train_theta = np.reshape(X_train.dot(theta.T), [-1])\n",
        "        sigma = expit(-Y_train * x_train_theta)\n",
        "\n",
        "        d_theta = slin.lu_solve(cov_lu_factor, X_train.T).T\n",
        "\n",
        "        quad_x = np.sum(X_train * d_theta, axis=1)\n",
        "\n",
        "        return sigma**2 * quad_x"
      ],
      "metadata": {
        "id": "0k6bzco5_5hY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "cols = ['status', 'duration', 'credit_hist', 'purpose', 'credit_amt', 'savings', 'employment', 'installment_rate', 'personal_status', 'debtors', 'residencesince', 'property', 'age', 'install_plans', 'housing', 'existing_credits', 'job', 'maintenance_paying_people', 'telephone', 'foreign_worker', 'result']\n",
        "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data'\n",
        "df = pd.read_csv(url, delimiter=' ', names=cols, index_col=False)\n",
        "# df = load_dataset(filepath)\n",
        "\n",
        "# df = pd.read_table('german.data', names=cols, sep=\" \", index_col=False)"
      ],
      "metadata": {
        "id": "gSqPDsIXWeOy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_german(df):\n",
        "    df['status'] = df['status'].map({'A11': 0, 'A12': 1, 'A13': 2, 'A14': 3}).astype(int)\n",
        "    df['credit_hist'] = df['credit_hist'].map({'A34': 0, 'A33': 1, 'A32': 2, 'A31': 3, 'A30': 4}).astype(int)\n",
        "    df.loc[(df['credit_amt'] <= 2000), 'credit_amt'] = 0\n",
        "    df.loc[(df['credit_amt'] > 2000) & (df['credit_amt'] <= 5000), 'credit_amt'] = 1\n",
        "    df.loc[(df['credit_amt'] > 5000), 'credit_amt'] = 2\n",
        "    df.loc[(df['duration'] <= 12), 'duration'] = 0\n",
        "    df.loc[(df['duration'] > 12) & (df['duration'] <= 24), 'duration'] = 1\n",
        "    df.loc[(df['duration'] > 24) & (df['duration'] <= 36), 'duration'] = 2\n",
        "    df.loc[(df['duration'] > 36), 'duration'] = 3\n",
        "    df['age'] = df['age'].apply(lambda x : 1 if x >= 45 else 0) # 1 if old, 0 if young\n",
        "    df['savings'] = df['savings'].map({'A61': 0, 'A62': 1, 'A63': 2, 'A64': 3, 'A65': 4}).astype(int)\n",
        "    df['employment'] = df['employment'].map({'A71': 0, 'A72': 1, 'A73': 2, 'A74': 3, 'A75': 4}).astype(int)\n",
        "    df['gender'] = df['personal_status'].map({'A91': 1, 'A92': 0, 'A93': 1, 'A94': 1, 'A95': 0}).astype(int)\n",
        "    df['debtors'] = df['debtors'].map({'A101': 0, 'A102': 1, 'A103': 2}).astype(int)\n",
        "    df['property'] = df['property'].map({'A121': 3, 'A122': 2, 'A123': 1, 'A124': 0}).astype(int)\n",
        "    df['install_plans'] = df['install_plans'].map({'A141': 1, 'A142': 1, 'A143': 0}).astype(int)\n",
        "    df['job'] = df['job'].map({'A171': 0, 'A172': 1, 'A173': 2, 'A174': 3}).astype(int)\n",
        "    df['telephone'] = df['telephone'].map({'A191': 0, 'A192': 1}).astype(int)\n",
        "    df['foreign_worker'] = df['foreign_worker'].map({'A201': 1, 'A202': 0}).astype(int)\n",
        "    pd.get_dummies(df, columns=['purpose', 'housing'], drop_first=True)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "KX4OG9aWYKuo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = preprocess_german(df);"
      ],
      "metadata": {
        "id": "T2bkoIyaYP1s"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop([\"purpose\", \"housing\", \"personal_status\"], axis=1)"
      ],
      "metadata": {
        "id": "h8iw_inpYXFw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = df[['status', 'duration', 'credit_hist', 'credit_amt', 'savings',\n",
        "       'employment', 'installment_rate', 'debtors', 'residencesince',\n",
        "       'property', 'age', 'install_plans', 'existing_credits', 'job',\n",
        "       'maintenance_paying_people', 'telephone', 'foreign_worker',\n",
        "       'gender']]\n",
        "\n",
        "y = df[[\"result\"]]"
      ],
      "metadata": {
        "id": "WaWI2PvDYhol"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state=15)"
      ],
      "metadata": {
        "id": "9amS1o9NYlmr"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the dataset\n",
        "preprocessed_df = preprocess_german(df)\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "# Calculate LOO influences\n",
        "influences = get_loo_influences(model, x_train, y_train)"
      ],
      "metadata": {
        "id": "TBFQ1BNYKR56"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}